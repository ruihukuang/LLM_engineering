# Week 3  

## day 2  

Context  

Use T4 GPU in the Google Colab to run pipelines to do the following tasks.  
sentiment-analysis   
named entity recognition  
question answering with provided contexts  
text summarization  
translation from English to France  
translation from English to Spanish  
classifying a sentence based on provided categories  
generation of remaining words based on provided words in a sentence  
creation of an image based on provided descriptions  
generating an audio file based on provided words    


Screenshots show outputs for these tasks.  



## day 3  

Context  

Use the following models to tokenize input messages.  

meta-llama/Meta-Llama-3.1-8B 
meta-llama/Meta-Llama-3.1-8B-Instruct  
microsoft/Phi-3-mini-4k-instruct  
Qwen/Qwen2-7B-Instruct  
bigcode/starcoder2-3b  


## day 4  

Context 

Use the following models to tokenize input messages and then to use system/user messages to ask these models to tell a light-heared joke for data scientists.  

meta-llama/Meta-Llama-3.1-8B-Instruct  
microsoft/Phi-3-mini-4k-instruct 
google/gemma-2-2b-it  

Screenshots below show outputs.  

A joke from meta-llama/Meta-Llama-3.1-8B-Instruct  
<img width="1946" height="488" alt="image" src="https://github.com/user-attachments/assets/f7d316bb-11e2-4da2-888f-f2e1b54724e6" /> 

A joke from microsoft/Phi-3-mini-4k-instruct    
<img width="2574" height="137" alt="image" src="https://github.com/user-attachments/assets/69aa512b-216a-4b3f-99b5-af80cbcc537d" />  

A joke from google/gemma-2-2b-it  
<img width="1233" height="379" alt="image" src="https://github.com/user-attachments/assets/d7ab8ce9-748e-43fd-96fd-520d7ca22e9c" />  









